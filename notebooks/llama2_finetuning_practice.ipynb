{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/getachew_abebe/LLM_Fine-Tunning_Amharic/llm_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from utils import calculate_length, preprocess_article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "2d8601d8fa6949dd90a7555108e82f7e"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-05-30T14:20:22.516890Z",
          "iopub.status.busy": "2024-05-30T14:20:22.515888Z",
          "iopub.status.idle": "2024-05-30T14:20:32.271734Z",
          "shell.execute_reply": "2024-05-30T14:20:32.270988Z",
          "shell.execute_reply.started": "2024-05-30T14:20:22.516854Z"
        },
        "id": "H-AfF9KLiJnz",
        "outputId": "3dcee90a-4864-4339-9871-af30c5d7b3c1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "data = load_dataset(\"csv\", data_files=\"/home/getachew_abebe/LLM_Fine-Tunning_Amharic/data/Amharic.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['train'] = data['train'].map(calculate_length, batched=False)\n",
        "data['train'] = data['train'].map(preprocess_article, batched=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model\n",
        "base_model = \"NousResearch/Llama-2-7b-hf\"\n",
        "new_model = \"llama-2-7b-Amharic\"\n",
        "# Dataset\n",
        "dataset = data\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.94s/it]\n",
            "/home/getachew_abebe/LLM_Fine-Tunning_Amharic/llm_env/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/getachew_abebe/LLM_Fine-Tunning_Amharic/llm_env/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/getachew_abebe/LLM_Fine-Tunning_Amharic/llm_env/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/home/getachew_abebe/LLM_Fine-Tunning_Amharic/llm_env/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        ")\n",
        "\n",
        "# Load base moodel\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "# Cast the layernorm in fp32, make output embedding layer require grads, add the upcasting of the lmhead to fp32\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['train']\n"
          ]
        }
      ],
      "source": [
        "print(list(dataset.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T14:20:32.274101Z",
          "iopub.status.busy": "2024-05-30T14:20:32.273664Z",
          "iopub.status.idle": "2024-05-30T14:20:32.280714Z",
          "shell.execute_reply": "2024-05-30T14:20:32.279815Z",
          "shell.execute_reply.started": "2024-05-30T14:20:32.274075Z"
        },
        "id": "pvVE6jzxiJn-",
        "outputId": "81341fb5-8427-44a3-f9cd-ac2a4a519749",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/getachew_abebe/LLM_Fine-Tunning_Amharic/llm_env/lib/python3.8/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "/home/getachew_abebe/LLM_Fine-Tunning_Amharic/llm_env/lib/python3.8/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/home/getachew_abebe/LLM_Fine-Tunning_Amharic/llm_env/lib/python3.8/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 : < :, Epoch 0.00/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set training arguments\n",
        "training_arguments = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=10,\n",
        "        gradient_accumulation_steps=1,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=1000,\n",
        "        logging_steps=1,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        learning_rate=2e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        warmup_steps=10,\n",
        "        # report_to=\"wandb\",\n",
        "        max_steps=2, # Remove this line for a real fine-tuning\n",
        "        logging_strategy=\"steps\",  # <--- Add this line\n",
        "        save_strategy=\"steps\",  # <--- Add this line\n",
        "\n",
        ")\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['train'],\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"article\",\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "# # Access the training and evaluation losses\n",
        "# train_loss = trainer.state.log_history[-1]['train_loss']  # Last training loss\n",
        "# eval_loss = trainer.state.log_history[-1]['eval_loss']   # Last evaluation loss\n",
        "\n",
        "# print(f\"Training Loss: {train_loss}\")\n",
        "# print(f\"Evaluation Loss: {eval_loss}\")\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'DatasetDict' object has no attribute 'describe'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribe\u001b[49m()\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'describe'"
          ]
        }
      ],
      "source": [
        "dataset['article']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T14:21:25.789869Z",
          "iopub.status.busy": "2024-05-30T14:21:25.788920Z",
          "iopub.status.idle": "2024-05-30T14:21:26.010834Z",
          "shell.execute_reply": "2024-05-30T14:21:26.009892Z",
          "shell.execute_reply.started": "2024-05-30T14:21:25.789832Z"
        },
        "id": "qZc1187yiJoB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "category = list(set(data['train']['category']))\n",
        "\n",
        "checkpoint = \"meta-llama/Llama-2-7b-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Tokenize the dataset\n",
        "category_to_id = {cat: idx for idx, cat in enumerate(category)}\n",
        "\n",
        "def tokenize_function(example):\n",
        "    inputs = tokenizer(example['article'], padding=True, truncation=True, max_length=512)\n",
        "    inputs[\"labels\"] = category_to_id[example[\"category\"]]  # Assuming category is already integer-encoded\n",
        "    return inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "bca72e4e593d4a1382d2d9b16fe37eaf",
            "f0073336b200468f9e14d05d0be56b2b",
            "5f82fce25f494425926be0ab0e5183a0",
            "e16efbd58a6241169acd66631268dd13",
            "8ceba03ca79941a398ad5657bbe765c7",
            "48a07e19134e4a1eb7040aa02bbfedcb"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-05-30T14:21:26.012250Z",
          "iopub.status.busy": "2024-05-30T14:21:26.011969Z",
          "iopub.status.idle": "2024-05-30T14:24:07.089243Z",
          "shell.execute_reply": "2024-05-30T14:24:07.088286Z",
          "shell.execute_reply.started": "2024-05-30T14:21:26.012219Z"
        },
        "id": "-6TPC4usiJoC",
        "outputId": "40af8d5f-47b4-4c22-9c8d-3ad6992cb43a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 49532/49532 [03:15<00:00, 253.63 examples/s]\n",
            "Map: 100%|██████████| 12383/12383 [00:48<00:00, 254.55 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['article', 'category', 'word_count', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 49532\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['article', 'category', 'word_count', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 12383\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tokenized_datasets = raw_datasets.map(tokenize_function)\n",
        "# Use a data collator to apply dynamic batches\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='pt')\n",
        "\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "print(tokenized_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'article': 'ሶማልያ የሚገኘውና ሞቅዲሾ ስታዲየም ውስጥ ሰፍሮ የነበረው የተመድ ሚሽን አሚሶም ከስፖርት ሜዳው ለቆ መውጣቱ ተገለፀአፍሪካ ህብረት ሚሽን በመጨረሻ የወጣቱን ድምፅ ሰምቶ ብሄራዊው ስታዲየም እንደገና ለስፖርቱ እንቅስቃሴ እንዲውል ሜዳውን በመልቀቁ እጅግ ተደስተናል ሲሉ የወጣቶችና ስፖርት ሚኒስትሯ ከሀዲጆ ሞሀመድ በርክክቡ ስነ ስርአት ላይ ተናግረዋልየሶማልያው ፕሬዚደንት ሞሀመድ አብዱላሂ ፎርማጆ በበኩላቸው መንግስት የጦር ቀጣና ሆኖ የቆየውንና በእጅጉ የተጎዳውን ስታዲየም ለሀገሪቱ የስፖርት እንቅስቃሴ ለማዋል ጥረት እንደሚያደርግ አስታውቀዋልይህ እአአ በ1970ዎቹ በቻይናውያን የተገነባው ታዲየም ከወታደራዊ ልምምድ ሌላ ምንም አይነት የአትሌቲክስ እንቅስቃሴ እንዳላስተናገደ ይታወቃል',\n",
              " 'category': 'International News',\n",
              " 'word_count': tensor(443),\n",
              " 'input_ids': tensor([    1, 29871,   228,   139,   185,   228,   139,   158,   228,   139,\n",
              "           144,   228,   142,   174, 29871,   228,   142,   171,   228,   139,\n",
              "           157,   228,   143,   139,   228,   141,   155,   228,   142,   144,\n",
              "           228,   141,   150, 29871,   228,   139,   161,   228,   140,   136,\n",
              "           228,   142,   181,   228,   139,   193, 29871,   228,   139,   184,\n",
              "           228,   140,   182,   228,   142,   181,   228,   142,   171,   228,\n",
              "           139,   160, 29871,   228,   142,   144,   228,   139,   184,   228,\n",
              "           143,   168, 29871,   228,   139,   179,   228,   144,   144,   228,\n",
              "           139,   177, 29871,   228,   142,   171,   228,   141,   147,   228,\n",
              "           140,   163,   228,   139,   171,   228,   142,   144, 29871,   228,\n",
              "           142,   171,   228,   140,   179,   228,   139,   155,   228,   142,\n",
              "           184, 29871,   228,   139,   157,   228,   139,   192,   228,   141,\n",
              "           152, 29871,   228,   141,   163,   228,   139,   157,   228,   139,\n",
              "           185,   228,   139,   160, 29871,   228,   141,   171,   228,   139,\n",
              "           184,   228,   144,   153,   228,   139,   176,   228,   140,   184,\n",
              "         29871,   228,   139,   159,   228,   142,   182,   228,   142,   144,\n",
              "         29871,   228,   139,   139,   228,   140,   137, 29871,   228,   139,\n",
              "           155,   228,   142,   144,   228,   143,   166,   228,   140,   180,\n",
              "         29871,   228,   140,   179,   228,   143,   139,   228,   139,   139,\n",
              "           228,   144,   131,   228,   141,   163,   228,   144,   144,   228,\n",
              "           139,   173,   228,   141,   174, 29871,   228,   139,   136,   228,\n",
              "           140,   168,   228,   139,   171,   228,   140,   184, 29871,   228,\n",
              "           139,   157,   228,   139,   192,   228,   141,   152, 29871,   228,\n",
              "           140,   163,   228,   139,   155,   228,   143,   171,   228,   139,\n",
              "           171,   228,   139,   190, 29871,   228,   142,   171,   228,   142,\n",
              "           139,   228,   143,   166,   228,   140,   180,   228,   141,   152,\n",
              "         29871,   228,   142,   184,   228,   139,   160,   228,   144,   136,\n",
              "         29871,   228,   139,   179,   228,   139,   160,   228,   140,   185,\n",
              "         29871,   228,   140,   168,   228,   139,   135,   228,   139,   174,\n",
              "           228,   142,   141,   228,   142,   144, 29871,   228,   139,   184,\n",
              "           228,   140,   182,   228,   142,   181,   228,   142,   171,   228,\n",
              "           139,   160, 29871,   228,   141,   168,   228,   141,   152,   228,\n",
              "           142,   179,   228,   143,   139,   228,   141,   150, 29871,   228,\n",
              "           139,   139,   228,   139,   184,   228,   144,   153,   228,   139,\n",
              "           176,   228,   140,   180, 29871,   228,   141,   168,   228,   141,\n",
              "           152,   228,   140,   136,   228,   139,   184,   228,   140,   134,\n",
              "           228,   139,   183, 29871,   228,   141,   168,   228,   141,   152,\n",
              "           228,   142,   181,   228,   142,   144,   228,   139,   144, 29871,\n",
              "           228,   139,   159,   228,   142,   182,   228,   142,   144,   228,\n",
              "           141,   152, 29871,   228,   140,   163,   228,   139,   155,   228,\n",
              "           139,   144,   228,   140,   131,   228,   140,   132, 29871,   228,\n",
              "           141,   168,   228,   143,   136,   228,   143,   144, 29871,   228,\n",
              "           140,   179,   228,   142,   179,   228,   139,   184,   228,   140,\n",
              "           179,   228,   141,   150,   228,   139,   144, 29871,   228,   139,\n",
              "           181,   228,   139,   140, 29871,   228,   142,   171,   228,   142,\n",
              "           139,   228,   143,   166,   228,   140,   185,   228,   140,   192,\n",
              "           228,   141,   150, 29871,   228,   139,   184,   228,   144,   153,\n",
              "           228,   139,   176,   228,   140,   184, 29871,   228,   139,   157,\n",
              "           228,   141,   149,   228,   139,   184,   228,   140,   184,   228,\n",
              "           139,   178, 29871,   228,   141,   171,   228,   139,   131,   228,\n",
              "           142,   181,   228,   143,   137, 29871,   228,   139,   161,   228,\n",
              "           139,   131]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
              " 'labels': tensor(1)}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets['train'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6a9fe9f473a24e91a575f2bcd3e8a4b9",
            "35ac89a9aca641fe930e7b50fdf96e36"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-05-30T14:24:07.090970Z",
          "iopub.status.busy": "2024-05-30T14:24:07.090583Z",
          "iopub.status.idle": "2024-05-30T14:24:10.156559Z",
          "shell.execute_reply": "2024-05-30T14:24:10.155296Z",
          "shell.execute_reply.started": "2024-05-30T14:24:07.090934Z"
        },
        "id": "sdlDkDQSiJoE",
        "outputId": "6af71da9-2366-4163-a583-295cbfdad6b6",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading shards: 100%|██████████| 2/2 [00:19<00:00,  9.90s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.44s/it]\n",
            "Some weights of Phi3ForSequenceClassification were not initialized from the model checkpoint at microsoft/Phi-3-mini-128k-instruct and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load the model\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# roberta-base\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    checkpoint,\n",
        "    num_labels=len(category),\n",
        "    id2label = {i: lbl for i, lbl in enumerate(category)},\n",
        "    label2id = {lbl: i for i, lbl in enumerate(category)},\n",
        "    device_map=\"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T14:24:10.161828Z",
          "iopub.status.busy": "2024-05-30T14:24:10.161388Z",
          "iopub.status.idle": "2024-05-30T14:24:10.224241Z",
          "shell.execute_reply": "2024-05-30T14:24:10.223476Z",
          "shell.execute_reply.started": "2024-05-30T14:24:10.161791Z"
        },
        "id": "1z1onqOfiJoG",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=checkpoint+\"-finetuned\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=0.1,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    fp16=True,\n",
        "    seed=42,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T15:16:46.595651Z",
          "iopub.status.busy": "2024-05-30T15:16:46.595273Z",
          "iopub.status.idle": "2024-05-30T15:16:48.597991Z",
          "shell.execute_reply": "2024-05-30T15:16:48.596975Z",
          "shell.execute_reply.started": "2024-05-30T15:16:46.595619Z"
        },
        "id": "6FvPjfgyiJoI",
        "outputId": "532a6c38-c23f-4a22-b71d-f0f21285860e",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "  metric1 = evaluate.load(\"accuracy\")\n",
        "  metric2 = evaluate.load(\"precision\")\n",
        "  metric3 = evaluate.load(\"recall\")\n",
        "  metric4 = evaluate.load(\"f1\")\n",
        "\n",
        "  logits, labels = eval_preds\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "  accuracy = metric1.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "  precision = metric2.compute(predictions=predictions, references=labels, average='weighted')[\"precision\"]\n",
        "  recall = metric3.compute(predictions=predictions, references=labels, average='weighted')[\"recall\"]\n",
        "  f1 = metric4.compute(predictions=predictions, references=labels, average='weighted')[\"f1\"]\n",
        "\n",
        "  return {\n",
        "      \"accuracy\": accuracy,\n",
        "      \"precision\": precision,\n",
        "      \"recall\": recall,\n",
        "      \"f1\": f1\n",
        "  }\n",
        "\n",
        "compute_metrics(([[1,0], [0,1]], [0,1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T15:16:54.683357Z",
          "iopub.status.busy": "2024-05-30T15:16:54.682462Z",
          "iopub.status.idle": "2024-05-30T15:50:06.480852Z",
          "shell.execute_reply": "2024-05-30T15:50:06.479563Z",
          "shell.execute_reply.started": "2024-05-30T15:16:54.683302Z"
        },
        "id": "E2yiwdpxiJoK",
        "outputId": "54250398-5319-4951-d462-7ec2c0956e00",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T15:50:26.786895Z",
          "iopub.status.busy": "2024-05-30T15:50:26.786129Z",
          "iopub.status.idle": "2024-05-30T15:51:03.314295Z",
          "shell.execute_reply": "2024-05-30T15:51:03.312869Z",
          "shell.execute_reply.started": "2024-05-30T15:50:26.786857Z"
        },
        "id": "cuRDqxtHiJoL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Load metrics and evaluate the model\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "eval_dataset = tokenized_datasets[\"test\"].remove_columns([\n",
        "    'article', 'category', 'word_count'\n",
        "    ]).with_format(\"torch\")\n",
        "\n",
        "\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_dataset,\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "y_test, y_pred = [], []\n",
        "model.eval()\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to('cuda') for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    y_pred.extend(predictions.cpu().numpy())\n",
        "    y_test.extend(batch[\"labels\"].cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T15:51:26.898857Z",
          "iopub.status.busy": "2024-05-30T15:51:26.898391Z",
          "iopub.status.idle": "2024-05-30T15:51:26.909514Z",
          "shell.execute_reply": "2024-05-30T15:51:26.907789Z",
          "shell.execute_reply.started": "2024-05-30T15:51:26.898823Z"
        },
        "id": "aADOOG8wiJoM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(y_pred, y_test):\n",
        "  metric1 = evaluate.load(\"accuracy\")\n",
        "  metric2 = evaluate.load(\"precision\")\n",
        "  metric3 = evaluate.load(\"recall\")\n",
        "  metric4 = evaluate.load(\"f1\")\n",
        "\n",
        "  #logits, labels = y_preds\n",
        "  #predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "  accuracy = metric1.compute(predictions=y_pred, references=y_test)[\"accuracy\"]\n",
        "  precision = metric2.compute(predictions=y_pred, references=y_test, average='weighted')[\"precision\"]\n",
        "  recall = metric3.compute(predictions=y_pred, references=y_test, average='weighted')[\"recall\"]\n",
        "  f1 = metric4.compute(predictions=y_pred, references=y_test, average='weighted')[\"f1\"]\n",
        "\n",
        "  return {\n",
        "      \"accuracy\": accuracy,\n",
        "      \"precision\": precision,\n",
        "      \"recall\": recall,\n",
        "      \"f1\": f1\n",
        "  }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDib-iggiJr5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T15:03:31.915215Z",
          "iopub.status.busy": "2024-05-30T15:03:31.914398Z",
          "iopub.status.idle": "2024-05-30T15:03:35.144089Z",
          "shell.execute_reply": "2024-05-30T15:03:35.142956Z",
          "shell.execute_reply.started": "2024-05-30T15:03:31.915180Z"
        },
        "id": "gLela_lIiJr_",
        "outputId": "024dde76-0421-4097-9b55-2a0bfad7b679",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.875232173140596,\n",
              " 'precision': 0.8749618364453737,\n",
              " 'recall': 0.875232173140596,\n",
              " 'f1': 0.8748733706909614}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_metrics(y_pred, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T15:51:38.585611Z",
          "iopub.status.busy": "2024-05-30T15:51:38.585188Z",
          "iopub.status.idle": "2024-05-30T15:51:42.154374Z",
          "shell.execute_reply": "2024-05-30T15:51:42.153158Z",
          "shell.execute_reply.started": "2024-05-30T15:51:38.585576Z"
        },
        "id": "dr6dwyk_iJsB",
        "outputId": "b43f8583-c2e5-4a47-b4ac-17af5913b0fb",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.875232173140596,\n",
              " 'precision': 0.8749618364453737,\n",
              " 'recall': 0.875232173140596,\n",
              " 'f1': 0.8748733706909614}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_metrics(y_pred, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-30T15:11:45.222966Z",
          "iopub.status.busy": "2024-05-30T15:11:45.222582Z",
          "iopub.status.idle": "2024-05-30T15:11:46.213107Z",
          "shell.execute_reply": "2024-05-30T15:11:46.212082Z",
          "shell.execute_reply.started": "2024-05-30T15:11:45.222936Z"
        },
        "id": "GjfUndBciJsD",
        "outputId": "eaedc264-a8b3-455b-9715-a3c85a968f0f",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'f1': 0.8748733706909614}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metric = evaluate.load(\"f1\")\n",
        "metric.compute(predictions=y_pred, references=y_test, average='weighted')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 3141921,
          "sourceId": 5495819,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30699,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
